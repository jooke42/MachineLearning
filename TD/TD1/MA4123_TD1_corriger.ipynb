{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD Vectorisation de texte et représentation graphique\n",
    "\n",
    "Dans ce TD, nous nous intéressons à la représentation vectorielle de texte à partir de la méthode [Bag of words](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Nous étudierons les effets des différents paramètres sur la représentation obtenue. Nous étudierons également deux méthodes pour représenter graphiquement des données vectorielles de grande dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys,os,os.path\n",
    "os.environ['HTTP_PROXY']=\"http://cache.esiee.fr:3128\"\n",
    "os.environ['HTTPS_PROXY']=\"http://cache.esiee.fr:3128\"\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'ensemble de textes : 20 newsgroups\n",
    "\n",
    "Dans ce TD, nous utiliserons l'ensemble de textes qui se nomme « 20 newsgroups ». C'est une collection d'environ 20000 documents de groupes de discussion, répartis (presque) uniformément dans 20 groupes de discussion différents. Cet ensemble de textes est devenu populaire pour les expériences de Machine learning dans les applications de texte, telles que la classification de texte et le regroupement de texte.\n",
    "\n",
    "Pour plus de detaille : http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Chargez un échantillon de l'ensemble de textes avec les paramètres suivants ([sklearn.datasets.fetch_20newsgroups()](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html)) :\n",
    "- uniquement les catégories : 'alt.atheism', 'soc.religion.christian','comp.graphics' et 'sci.med'\n",
    "- le sous-ensemble d'entrainement ('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories=['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `twenty_train`, contient les 3 sous-variables suivantes :\n",
    "- **`twenty_train.target_names`** : la liste des noms de sujet\n",
    "- **`twenty_train.target`** : la liste des indexs des sujets pour chaque message\n",
    "- **`twenty_train.data`** : la liste des messages\n",
    "\n",
    "**Question -** Afficher la liste des des noms de sujet, les noms de sujet des 20 premiers messages et le texte de quelques messages ([print()](https://docs.python.org/3/tutorial/inputoutput.html), [string.join()](https://docs.python.org/3.5/library/string.html), [repr()](https://docs.python.org/3.5/library/functions.html?highlight=repr#repr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Liste des noms de sujet : ' + ', '.join(twenty_train.target_names))\n",
    "print()\n",
    "print('Sujet des 20 premiers messages : ' + ', '.join([twenty_train.target_names[twenty_train.target[i]] for i in range(20)]))\n",
    "print()\n",
    "print('Exemple de message :\\n' + '\\n\\n'.join([repr(twenty_train.data[i]) for i in (1,25,256)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Que remarquez-vous sur le contenu des messages (Attention ! Utiliser la fonction **`repr()`**) ? Peut-on les utiliser directement sous cette forme ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of words\n",
    "### Création du dictionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Créez un `CountVectorizer` et utilisez le corpus de texte pour apprendre le dictionnaire de vectorisation ([CountVectorizer.fit()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(twenty_train.data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Quelle est la taille du dictionnaire obtenu (`CountVectorizer.vocabulary_`) ? Affichez les 500 premiers mots du dictionnaire, que pouvez-vous en conclure ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Nombre de mots dans le disctionnaire :', len(vectorizer.vocabulary_))\n",
    "print(' '.join([list(vectorizer.vocabulary_.keys())[i] for i in range(1000)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Écrivez la fonction `preprocess_text(mess)`, qui permet de prétraiter les textes ([str.replace()](https://docs.python.org/3.5/library/stdtypes.html?highlight=replace#str.replace))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_text(mess):\n",
    "    mess = mess.replace('_',' ')\n",
    "    return mess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous surchargons la fonction `build_analyzer` de la classe `CountVectorizer` pour y ajouter notre prétraitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CountVectorizerV2(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(CountVectorizerV2, self).build_analyzer()\n",
    "        return lambda doc: analyzer(preprocess_text(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Créez un `CountVectorizerV2` et utilisez le corpus de texte pour apprendre le dictionnaire de vectorisation ([CountVectorizer.fit()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.fit)).\n",
    "\n",
    "Quelle est la nouvelle taille du dictionnaire obtenu ? Affichez les nouveaux 500 premiers mots du dictionnaire, cela vous semble-t-il mieux ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizerV2()\n",
    "vectorizer.fit(twenty_train.data)\n",
    "print('Nombre de mots dans le disctionnaire :', len(vectorizer.vocabulary_))\n",
    "print(' '.join([list(vectorizer.vocabulary_.keys())[i] for i in range(1000)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Réduction du nombre de mots dans le dictionnaire\n",
    "\n",
    "Pour réduire le nombre de mots dans le dictionnaire, nous allons étudier la fréquence d'apparition des mots dans les documents  *document frequency* en anglais.\n",
    "\n",
    "**Question -** Utilisez la fonction [CountVectorizer.transform()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.transform) pour vectoriser les documents du corpus de texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(twenty_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Ecrivez la fonction `compute_document_frequency` qui calcule la fréquence d'apparition de chaque mot dans les documents ([numpy.mean()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.mean.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_document_frequency(X):\n",
    "    \n",
    "    df = np.mean(X>0, axis=0)\n",
    "    df = np.asarray(df).reshape(-1) # convertie un numpy.matrix en numpy.ndarray\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Affichez l'histogramme de la fréquence d'apparition des mots dans les documents du corpus de texte ([matplotlib.pyplot.hist()](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = compute_document_frequency(X)\n",
    "\n",
    "fig = plt.figure();\n",
    "ax = fig.gca();\n",
    "ax.hist(df,100,normed=True);\n",
    "ax.set_yscale('log');\n",
    "ax.set_xbound([0,1]);\n",
    "ax.set_xlabel('Document frequency');\n",
    "ax.set_ylabel('Frequenc of document frequency');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Qu'est-ce que cela signifie ? Est-il intéressant d'avoir des mots avec une fréquence proche de 1 ou de 0 ? Quel est d'effet de ces mots sur la distance euclidienne entre la représentions vectorielle de deux textes ?\n",
    "\n",
    "**Question -** A l'aide du parametres `stop_words` de la classe [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), ajouter la liste des **stop_words** de la langue anglaise\n",
    "Quelle est la taille du dictionnaire alors obtenu ?\n",
    "Afficher alors le nouvelle l'histogramme de la fréquence d'apparition des mots dans les documents du corpus de texte ([matplotlib.pyplot.hist()](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist)).\n",
    "\n",
    "Que remarquez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizerV2(stop_words='english')\n",
    "vectorizer.fit(twenty_train.data)\n",
    "print('Nombre de mots dans le disctionnaire :', len(vectorizer.vocabulary_))\n",
    "\n",
    "X = vectorizer.transform(twenty_train.data)\n",
    "df = compute_document_frequency(X)\n",
    "\n",
    "fig = plt.figure();\n",
    "ax = fig.gca();\n",
    "ax.hist(df,100,normed=True);\n",
    "ax.set_yscale('log');\n",
    "ax.set_xbound([0,1]);\n",
    "ax.set_xlabel('Document frequency');\n",
    "ax.set_ylabel('Frequenc of document frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons vu en cours qu'il existe pour chaque langue et pour un grand nombre de mots de cette langue un sous-ensemble de mots dérivés.  Pour réduire la taille du vocabulaire, nous conservons l'élément racine de ces sous ensembles.\n",
    "\n",
    "Pour cela nous ajoutons une étape de [racinisation](http://fr.wikipedia.org/wiki/Racinisation) à notre `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk.stem\n",
    "english_stemmer=nltk.stem.SnowballStemmer('english')\n",
    "class EnglishStemmedCountVectorizer(CountVectorizer): #EnglishStemmedCountVectorizer hérite de CountVectorizer\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(EnglishStemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(preprocess_text(doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** Avec notre nouveau `EnglishStemmedCountVectorizer`, quelle est la taille du dictionnaire alors obtenu ?\n",
    "Affichez alors le nouvel l'histogramme de la fréquence d'apparition des mots dans les documents du corpus de texte ([matplotlib.pyplot.hist()](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist)).\n",
    "\n",
    "Que remarquez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = EnglishStemmedCountVectorizer(stop_words='english')\n",
    "vectorizer.fit(twenty_train.data)\n",
    "print('Nombre de mots dans le disctionnaire :', len(vectorizer.vocabulary_))\n",
    "\n",
    "X = vectorizer.transform(twenty_train.data)\n",
    "df = compute_document_frequency(X)\n",
    "\n",
    "fig = plt.figure();\n",
    "ax = fig.gca();\n",
    "ax.hist(df,100,normed=True);\n",
    "ax.set_yscale('log');\n",
    "ax.set_xbound([0,1]);\n",
    "ax.set_xlabel('Document frequency');\n",
    "ax.set_ylabel('Frequenc of document frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** A l'aide de la fonction [CountVectorizer.inverse_transform()](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer.inverse_transform), quels sont les mots du dictionnaire qui ont une fréquence trop grande (par exemple avec un df > 0.25) ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Liste des mots trop fréquents dans les documents : ' + ', '.join(vectorizer.inverse_transform(df>0.25)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question -** A l'aide des parametres **`min_df`** et **`max_df`** de la classe [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), filtrer les mots avec des fréquences indésirables.\n",
    "\n",
    "Quelle est la taille du dictionnaire alors obtenu ?\n",
    "\n",
    "Affichez alors le nouvel l'histogramme de la fréquence d'apparition des mots dans les documents du corpus de texte ([matplotlib.pyplot.hist()](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.hist))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "min_df=0.015\n",
    "max_df=0.1\n",
    "\n",
    "vectorizer = EnglishStemmedCountVectorizer(stop_words='english',min_df=min_df, max_df=max_df)\n",
    "vectorizer.fit(twenty_train.data)\n",
    "print('Nombre de mots dans le disctionnaire :', len(vectorizer.vocabulary_))\n",
    "\n",
    "X = vectorizer.transform(twenty_train.data).toarray()\n",
    "df = compute_document_frequency(X)\n",
    "\n",
    "fig = plt.figure();\n",
    "ax = fig.gca();\n",
    "ax.hist(df,100,normed=True);\n",
    "#ax.set_yscale('log');\n",
    "ax.set_xbound([min_df,max_df]);\n",
    "ax.set_xlabel('Document frequency');\n",
    "ax.set_ylabel('Frequenc of document frequency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "Nous avons vu en cours qu'il est intéressant de visualiser les données pour juger de la qualité de la représentation. Il est possible de visualiser en regardant les dimensions de la représentation deux à deux avec la fonction suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_croises(X,y,target_names, threshold = 1.0):\n",
    "    nb_class=len(np.unique(y))\n",
    "    nb_feat=X.shape[1]\n",
    "    if nb_feat%2:\n",
    "        nrows=int((nb_feat-1)/2)\n",
    "        ncols=nb_feat\n",
    "    else:\n",
    "        nrows=int(nb_feat/2)\n",
    "        ncols=nb_feat-1\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    col=['b','r','g','c','m','y','k']\n",
    "    for t in range(nb_class):\n",
    "        k=1\n",
    "        for i in range(nb_feat-1):\n",
    "            for j in range(i+1,nb_feat):\n",
    "                plt.subplot(nrows,ncols,k)\n",
    "                \n",
    "                x_t = X[y == t,i]\n",
    "                y_t = X[y == t,j]\n",
    "                \n",
    "                x_mean = np.mean(x_t)\n",
    "                y_mean = np.mean(y_t)\n",
    "                \n",
    "                dist = (x_t-x_mean)**2 + (y_t-y_mean)**2\n",
    "                \n",
    "                \n",
    "                dist_max = np.sort(dist)[int(np.floor((dist.shape[0]-1)*threshold))]\n",
    "                \n",
    "                plt.scatter(x_t[dist<dist_max], y_t[dist<dist_max], color=col[t],lw=2,marker='o',label=target_names[t])\n",
    "                plt.ylabel('Dim. ' + str(j+1))\n",
    "                plt.xlabel('Dim. ' + str(i+1))\n",
    "                k=k+1\n",
    "    plt.legend(bbox_to_anchor=(2,2))  \n",
    "    plt.tight_layout()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons alors visualiser les 6 premières dimensions de notre représentation par Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_croises(X[:,:6],twenty_train.target,twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cela fait déjà beaucoup de graphiques à analyser alors que nous regardons que les 6 premières dimensions\n",
    "\n",
    "### Analyse en Composantes Principales\n",
    "\n",
    "Une méthodes tres couramments utilisée qui permet de visualiser les données est l'utilisation de [Analyse en Composantes Principales](https://fr.wikipedia.org/wiki/Analyse_en_composantes_principales)(ACP), qui est disponible dans [sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** A l'aide de l'ACP, réduisez la dimension de la signature à 6 dimensions et visualisez-les deux à deux avec la fonction `plot_croises()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=6)#,whiten=True\n",
    "pca.fit(X)\n",
    "X_redui = pca.transform(X)\n",
    "plot_croises(X_redui,twenty_train.target,twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Le résultat obtenu est-il lisible facilement lisible ? Pourquoi ? \n",
    "\n",
    "**Question -** Utilisez le paramètre `threshold` de la fonction d'affichage `plot_croises` pour supprimer les points trop éloignés de l'ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_croises(X_redui,twenty_train.target,twenty_train.target_names, threshold = 0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse ** Quelles sujets de discutions peuvent être facilement séparés ? Peut-on en conlure qu'il sera difficile de séparer les autres sujets ?\n",
    "\n",
    "## Discrimination par Plus Proches Voisins  (PPV)\n",
    "\n",
    "### Méthode des plus proches voisins\n",
    "Le principe est simple : pour une donnée dont on ne connait pas la classe mais seulement les caractéristiques, nous allons rechercher la donnée de la base complète (caractéristiques + classe) dont les caractéristiques sont les plus proches (au sens de la distance Euclidienne) et nous lui attribuerons la classe de cette donnée la plus proche.\n",
    "\n",
    "### Mise en oeuvre\n",
    "\n",
    "Les fonctions pour mettre en oeuvre la méthode des PPV et un code d'exemple sont donnés. Executez-les."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "def perf_knn(Xtrain,ytrain,Xtest,ytest,k=1):\n",
    "    knn=KNeighborsClassifier(k)\n",
    "    knn.fit(Xtrain, ytrain)\n",
    "    ctest=knn.predict(Xtest)\n",
    "    cm = confusion_matrix(ytest, ctest) \n",
    "    accuracy=sum(np.diag(cm))/sum(sum(cm))\n",
    "    erreur=1-accuracy\n",
    "    plot_confusion_matrix(cm,twenty_train.target_names)\n",
    "    print('exactitude = {0:.1%}.'.format(accuracy))\n",
    "    print('taux d''erreur = {0:.1%}.'.format(erreur))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories)\n",
    "\n",
    "X_test = vectorizer.transform(twenty_test.data).toarray()\n",
    "\n",
    "perf_knn(X,twenty_train.target,X_test,twenty_test.target,k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse -** Analysez la matrice de confusion obtenue, le résultat est-il cohérent avec la visualisation obtenue en utilisant l'ACP ?\n",
    "\n",
    "**Question -** Faites varier les parametres **`min_df`** et **`max_df`** de la classe [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) et observez les effets sur la visualisation et la matrice de confusion."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
