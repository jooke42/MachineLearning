{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# Table of Contents\n",
    "* [TP2 - Discrimination de bonnes ou mauvaises réponses dans une base de questions-réponses.](#TP2---Discrimination-de-bonnes-ou-mauvaises-réponses-dans-une-base-de-questions-réponses.)\n",
    "\t* [Première partie : récupérer les données](#Première-partie-:-récupérer-les-données)\n",
    "\t\t* [Récupération sur plusieurs années](#Récupération-sur-plusieurs-années)\n",
    "\t* [Deuxième partie: Trouver et définir des features](#Deuxième-partie:-Trouver-et-définir-des-features)\n",
    "\t\t* [Lecture de données](#Lecture-de-données)\n",
    "\t\t* [Construction de variables explicatives ](#Construction-de-variables-explicatives)\n",
    "\t* [Troisième partie: discrimination](#Troisième-partie:-discrimination)\n",
    "\t\t* [Première question](#Première-question)\n",
    "\t\t* [Seconde question](#Seconde-question)\n",
    "\t\t* [Tertio ](#Tertio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-53c678b79e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style>\n",
    "em {\n",
    "    color: green;\n",
    "}\n",
    "strong\n",
    "{\n",
    "    color: blue;\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "# TP2 (sujet) - Discrimination de bonnes ou mauvaises réponses dans une base de questions-réponses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "**Installation**: Vous téléchargerez le fichier [install_tp2](install_tp2)  dans un répertoire local qui vous va bien. Ceci étant fait, vous ouvrez un terminal, vous rendez dans ce répertoire, changez les droits d'accès au fichier par `chmod 755 install_tp2` de sorte à le rendre exécutable. Vous exécuterez ensuite le fichier par `./install_tp2`. Si tout va bien, il devrait créer un sous répertoire tp2, télécharger les documents et données utiles. Vous disposerez ensuite d'une commande `notebook` qui permet de lancer le notebook ipython après avoir reconfiguré 2 variables d'environnement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "![111](./Enluminure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "L'un des objectifs de ce TP est d'illustrer une méthode de discrimination, la **régression logistique**, sur des données internet réelles. Au passage, on verra également deux autres points importants dans l'analyse de données : \n",
    "- récupérer les données\n",
    "- extraire, construire, des variables explicatives de ces données (*feature extraction*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Les données auxquelles on s'intéressera sont *les réponses* fournies dans une base de type Stackoverflow. Plus exactement, on veut examiner des moyens d'évaluer la qualité des réponses fournies de manière automatique, avant même leur évaluation par des évaluateurs humains; éventuellement pour mettre en place une évaluation ou un guide automatique à la rédaction. Ici on ne prendra pas en compte le \"sens\", aussi l'exercice est particulièrement difficile, et il ne peut pas vraiment y avoir de miracle.  Voici quelques exemples de questions-réponses :\n",
    "\n",
    "- [Exemple de question avec réponse négative](http://stackoverflow.com/questions/886955/breaking-out-of-nested-loops-in-java)\n",
    "\n",
    "- [Un autre](http://stackoverflow.com/questions/3061/calling-a-function-of-a-module-from-a-string-with-the-functions-name-in-python)\n",
    "\n",
    "- [Et encore une](http://stackoverflow.com/questions/9001509/how-can-i-sort-a-python-dictionary-sort-by-key)\n",
    "\n",
    "- [Et une de plus](http://stackoverflow.com/questions/6797984/how-to-convert-string-to-lowercase-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "A partir de là, voici quelques exemples des questions que l'on peut se poser :\n",
    "\n",
    "- est-il possible de discriminer les réponses acceptées des non-acceptées ?\n",
    "- est-il possible de discriminer les réponses correctes (note >0) des autres ?\n",
    "- est-il possible de discriminer les réponses très bonnes réponses (note >10) des autres ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Le problème est très difficile, la base est très bruitée. Voyons donc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Première partie : récupérer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Assez sympatiquement, Stackexchange fournit une API pour interroger ses bases. L'adresse et la documentation de l'API est :\n",
    "[https://api.stackexchange.com/](https://api.stackexchange.com/)\n",
    "\n",
    "Pour sélectionner des données à récupérer sur Stack, on peut configurer la requête via la page suivante :\n",
    "\n",
    "[https://api.stackexchange.com/docs/advanced-search](https://api.stackexchange.com/docs/advanced-search)\n",
    "Cette page permet de configurer le filtrage des données et les champs renvoyés. ceci est encodé dans le paramètre `filter` de la requête. \n",
    "\n",
    "La requête suivante permet ainsi de récupérer les 9 dernières questions, avec une réponse acceptée, à propos d'ipython :\n",
    "\n",
    "https://api.stackexchange.com/docs/advanced-search#pagesize=9&order=desc&sort=activity&accepted=True&closed=True&tagged=ipython&filter=!*L1(ZTe*8)k0CMEL&site=stackoverflow&run=true "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> Faites le !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Les réponses peuvent être obtenues au format JSON (simplement en enlevant les deux dernières configs -- run et site). Par exemple, on utilisera\n",
    "\n",
    "https://api.stackexchange.com/2.2/search/advanced?pagesize=9&order=desc&sort=activity&accepted=True&closed=True&tagged=ipython&site=stackoverflow&filter=!*L1(ZTe*8)k0CMEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> Faites le !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Comme on le voit, le résultat est un fichier json, qui est similaire (mais pas totalement identique) à un dictionnaire Python. Pour récupérer des données, il suffit donc d'envoyer une série de reqêtes, de trier et d'enregistrer les réponses retournées. On se propose d'interrorger l'API avec le mot clé \"python\", entre deux dates, et de sauvegarder les réponses dans des fichiers csv. Commençons par le début : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Le module `requests` est un module Python pratique qui permet d'envoyer des requêtes hhtp et de récupérer le résultat. \n",
    "\n",
    "> Consultez l'aide de `requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "# demandez l'aide de ce module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">- en utilisant le module `requests` (`requests.get`, attribut `content` ), récupérez les données associées à la requête \n",
    "https://api.stackexchange.com/2.2/search/advanced?fromdate=1422748800&todate=1423440000&order=desc&sort=activity&accepted=True&tagged=ipython&site=stackoverflow&filter=!SlE.x1mh.L6ZoGtJtT\". Vous devrez décoder la réponse en utf8 en ajoutant un `.decode(\"utf8\")`.  \n",
    "Convertissez ensuite la réponse en dictionnaire à l'aide du module `json` (méthode `json.load`). Vous aurez besoin au passage d'utiliser `StringIO`, du module `io` qui permet d'associer un descripteur de fichier à un texte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import StringIO \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> Vous obtiendrez un dictionnaire, disons `dico_reponses`, que vous examinerez : afficher le dictionnaire, les clés, le contenu de la clé `items`, d'un élement de la clé items, par exemple `dico_reponses['items'][2]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Pour faire varier les dates, il faut convertir celles-ci en epoch Unix. Cela peut se faire par : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timezone\n",
    "print(\"date de maintenant: \", datetime.datetime.now())\n",
    "print(\"convertie :\",datetime.datetime.now().replace(tzinfo=timezone.utc).timestamp())\n",
    "#Pour convertir une date quelconque :\n",
    "print(datetime.datetime(2015,3,8).replace(tzinfo=timezone.utc).timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Sans enregistrement, il y une limite de 300 requêtes par jour. Pour disposer de quotas plus élevés, il faut demander une clé développeur, à l'adresse suivante : \n",
    "https://stackapps.com/users/login?returnurl=/apps/oauth/register.\n",
    "La clé correspondant à l'utilisateur \"Essais_ESIEE\"  (Client Id 441) est \n",
    "\n",
    "> Key ACNpxD0PS7)lMe*lkPPOWw((\n",
    "\n",
    "Ajouter un champ &key=ACNpxD0PS7)lMe*lkPPOWw((&  dans la requête."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Par ailleurs, Stackoverflow ne renvoie pas toutes les données, mais uniquement des \"pages\" (pour éviter les requêtes délirantes et la saturation de ses serveurs). La réponse contient une clé `has_more` qui si elle est True, dit qu'il faut passer à la page suivante. La requête peut contenir un paramètre `&page=`  qui permet d'indiquer cette page. Cette question n'est bien entendu pas essentielle, mais on vous invite à y réfléchir au moins un peu. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">- Ecrire les quelques lignes qui permettraient de récupérer les données entre deux dates spécifiées (y,m, d), . Stocker le résultat dans une liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "n=0\n",
    "topic=\"perl\"     # sujet de la requête (vous pouvez le changer !)\n",
    "todate=int(datetime.datetime(2015,3,10).replace(tzinfo=timezone.utc).timestamp())\n",
    "fromdate=int(datetime.datetime(2015,3,1).replace(tzinfo=timezone.utc).timestamp())\n",
    "qadict=dict()     # Dictionnaire dans lequel on lira la réponse\n",
    "stock=[]          # Liste dans laquelle on stockera les résultats\n",
    "## Si ans est la réponse à la requête, on peut convertir le json en dictionnaire python par\n",
    "# qadict=json.load(StringIO(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# A vous de compléter ici (décommenter le sligne sadéquates)\n",
    "#qadict['has_more']=True\n",
    "#while qadict['has_more']:\n",
    "#    n=n+1\n",
    "#    print(\"Itération {}\".format(n))\n",
    "    # A vous de compléter ici\n",
    "    # qadict=json.load(StringIO(ans))\n",
    "    # A vous de compléter ici\n",
    "#print(\"Fini\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Il faut ensuite stocker les résultats dans des fichiers. On se propose de le faire dans des fichiers de type csv. Une difficulté ici est que le nombre de réponses varie question par question, alors que le format d'un fichier csv est figé. Dans ce qui suit, on utilise un maximum de 6 réponses par question, initialisées à vide, et on stocke les différentes données. \n",
    "\n",
    "On va ensuite boucler sur le temps, en faisant varier les champs de date dans les requêtes, de manière à récupérer pas mal de données. On ne va pas vous le faire faire car cela prend pas mal de temps, à programmer comme à éxecuter (plusieurs jours), et ce n'est pas l'objectif principal du cours. \n",
    "\n",
    "Les fonctions suivantes sont donc fournies **à titre de documentation**. Les suggestions d'amélioration sont bienvenues. Vous êtes cependant invités à parcourir le code pour comprendre ce qui se passe. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Récupération sur plusieurs années"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Pour écrire les données dans un fichier csv, nous utilisons  les fonctions suivantes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "```python\n",
    "def cdico_init(nmax=6):\n",
    "    \"\"\"\n",
    "    Initialise le dictionnaire dans lequel on va écrire les différents champs des réponses\n",
    "    \"\"\"\n",
    "    cdico={}\n",
    "    body_keys = ['answer{}_body'.format(ii) for ii in range(1,nmax)]\n",
    "    score_keys =  ['answer{}_score'.format(ii) for ii in range(1,nmax)]\n",
    "    nbcomments_keys =  ['answer{}_nbcomments'.format(ii) for ii in range(1,nmax)]\n",
    "    for key in body_keys: cdico[key]=\"\"\n",
    "    for key in score_keys: cdico[key]=0\n",
    "    for key in nbcomments_keys: cdico[key]=0\n",
    "    cdico['accepted']=None    \n",
    "    return cdico\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "```python\n",
    "def save_qa(csvfilename,qa_dict,mode='wt',nmax=6):\n",
    "    \"\"\"\n",
    "    Fonction de sauvegarde des réponses json en un fichier csv de nom \"csvfilename\". \n",
    "    Par défaut, ce fichier est ouvert en mode \"wt\", mode texte, écriture et écrase si \n",
    "    le fichier existe. le nombre maximal de réponses est nmax. on utilise la méthode \n",
    "    DictWriter du module csv pour écrire le fichier. \n",
    "    \"\"\"\n",
    "    import csv\n",
    "    fieldnames=['creation_date','question_id', 'tags', 'question_title', 'question_body', 'answer_count', 'accepted']\n",
    "    for n in range(nmax):\n",
    "        fieldnames.append('answer{}_body'.format(n))\n",
    "        fieldnames.append('answer{}_score'.format(n))\n",
    "        fieldnames.append('answer{}_nbcomments'.format(n))\n",
    "\n",
    "    csvfile=open(csvfilename, mode, encoding='utf8')    \n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames,quoting=csv.QUOTE_ALL)\n",
    "    if mode=='wt': \n",
    "        print(\"writing header\")\n",
    "        writer.writeheader()    \n",
    "\n",
    "    for k in range(len(qa_dict['items'])):\n",
    "        #print(\"*\"*40)\n",
    "        if qa_dict['items'][k]['is_answered']:\n",
    "            cdico=cdico_init(nmax)\n",
    "            cdico['creation_date']= qa_dict['items'][k]['creation_date']\n",
    "            cdico['question_id']= qa_dict['items'][k]['question_id']\n",
    "            cdico['tags']= qa_dict['items'][k]['tags']\n",
    "            cdico['question_title']= qa_dict['items'][k]['title']\n",
    "            cdico['question_body']= qa_dict['items'][k]['body']\n",
    "            cdico['answer_count']= qa_dict['items'][k]['answer_count']\n",
    "            try:\n",
    "                answers=qa_dict['items'][k]['answers']        \n",
    "            except:\n",
    "                answers=[]\n",
    "            for n,answer in enumerate(answers):\n",
    "                if n>=nmax: break  \n",
    "                if answer['is_accepted']:\n",
    "                    cdico['accepted']=n\n",
    "                cdico['answer{}_body'.format(n)] =  answer['body']\n",
    "                cdico['answer{}_score'.format(n)] =  answer['score']\n",
    "                cdico['answer{}_nbcomments'.format(n)] =  answer['comment_count']\n",
    "            \n",
    "            writer.writerow(cdico)    \n",
    "    csvfile.close()        \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Boucle de lecture et de stockage des données sur plusieurs années (entre 2008 et 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "```python\n",
    "import requests            \n",
    "from io import StringIO \n",
    "import json\n",
    "import datetime, time\n",
    "\n",
    "#for topic in ('ipython', 'python'):\n",
    "for topic in ['python']:\n",
    "    #init date initiale\n",
    "    todate=int(datetime.datetime(2008,12,1).replace(tzinfo=timezone.utc).timestamp()) #\n",
    "    # Boucle sur années et mois ----------------------\n",
    "    for year in range(2008,2015): # 2009\n",
    "        for month in range(1,13):\n",
    "            print(\"#\"*60)\n",
    "            print(\"topic\", topic,\"year\",year)\n",
    "\n",
    "            fromdate=todate #int(datetime.datetime(year,1,1).replace(tzinfo=timezone.utc).timestamp()) #2011\n",
    "            todate=int(datetime.datetime(year,month,1).replace(tzinfo=timezone.utc).timestamp()) #2011\n",
    "            #nom du fichier de sauvegarde\n",
    "            csvfilename='qa_'+topic+'_'+str(datetime.date(year,month,1))+'.csv'\n",
    "            #autres inits\n",
    "            n=0\n",
    "            qadict=dict()\n",
    "            qadict['has_more']=True\n",
    "            #\n",
    "            # boucle sur les segnments de la réponse\n",
    "            #\n",
    "            while qadict['has_more']:\n",
    "                n=n+1\n",
    "                req=\"https://api.stackexchange.com/2.2/search/advanced?key=ACNpxD0PS7)lMe*lkPPOWw((&page={0}&fromdate={1}&todate={2}&order=desc&sort=activity&accepted=True&tagged={3}&site=stackoverflow&filter=!SlE.x1mh.L6ZoGtJtT\".format(n,fromdate,todate,topic)\n",
    "                r=requests.get(req)\n",
    "                ans=r.content\n",
    "                ans=ans.decode('utf8')\n",
    "                qadict=json.load(StringIO(ans))\n",
    "                        # save\n",
    "                print(\"Saving shrink n=\",n)\n",
    "                mode='wt' if n==1 else 'at'\n",
    "                save_qa(csvfilename,qadict,mode,nmax=6)\n",
    "                time.sleep(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Deuxième partie: Trouver et définir des features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Une étape très importante avant l'analyse des données proprement dite est d'extraire, de construire éventuellement, des variables qui permettront de distinguer les classes d'intérêt (variables explicatives). C'est ce qu'on se propose de faire dans cette partie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "L'un des fichiers générés est fourni ici : `qa_python_2014-11-01.csv` (réponses à la requête \"Python\" pour le mois de novembre 2014. Uniquement les questions comportant une réponse acceptée. 14 Mo ! L'ensemble des fichiers représente une taille de 655 Mo ! Chaque ligne contient la question posée, six des réponses apportées, avec leurs scores respectifs et les textes des réponses. C'est sur les textes es réponses que l'on va travailler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Lecture de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">A l'aide du module pandas, il est particulièrement simple de charger un fichier csv. Chargez le fichier csv précédent, sous le nom df,  en utilisant la méthode `pd.read_csv`. Enumérer les colonnes et affichez la description par respectivement `df.columns` et `df.describe()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# A vous de jouer ensuite\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Sélectionnons le texte de l'une des réponses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Pour sélectionner une donnée dans la dataframe `df`, on peut adresser d'abord la colonne, puis la ligne, suivant `df[label_de_la_colonne][index de la ligne]`.  \n",
    ">Afficher le score et le texte de la première réponse (answer0) correspondant à la 21e question. Sauvegardez le texte de la réponse dans une variable `tst_texte`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Construction de variables explicatives "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "A partir de ce texte, on veut construire différents indicateurs de \"qualité\". Des indicateurs de base seront bien-entendu\n",
    "\n",
    "- la longueur du texte, \n",
    "- nombre de mots, \n",
    "- le nombre de lignes,\n",
    "- le nombre de lignes de code,\n",
    "- les références (liens internet),\n",
    "- la mise en forme (gras, italique, etc).\n",
    "\n",
    "On pourrait y ajouter\n",
    "\n",
    "- nombre de mots en majuscule (mauvais style),\n",
    "- nombre d'images,\n",
    "- nombre de mots moyen par phrase...\n",
    "\n",
    "Les suggestions sont bienvenues. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Voici un exemple d'une telle fonction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def nb_paragraphs(text):\n",
    "    nb_p=len(re.findall(\"<p>\",text))\n",
    "    return nb_p\n",
    "\n",
    "##exemple\n",
    "#nb_paragraphs(tst_texte)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> - Choisissez deux ou trois de ces caractéristiques et écrivez les fonctions permettant d'extraire les paramètres associés. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Vous en profiterez pour manipuler les expressions régulières, qui ont été présentées (ou pas) lors du cours sur le [kit de survie](http://perso.esiee.fr/~bercherj/IT3007/Intro_Python.html#Expressions-régulières). L'exemple traité a été choisi en lien avec le problème qui nous occupe...\n",
    "\n",
    "En utilisant `re.findall` vous devriez pouvoir extraire au moins le nombre d'images, le nombre de liens, de nombre de mots en majuscules. Pour le nombre de mots, il suffit de faire un `.split()`, un split sur les \\n pour compter le nombre de lignes. Les autres sont un peu plus compliquées. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# fonction qui extrait calcule..\n",
    "# A vous de faire\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# fonction qui extrait calcule..\n",
    "# A vous de faire\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Le corrigé fournit une classe `answer_metrics` qui permet de calculer tout un tas de métriques associées à un texte. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">Importez la classe en question par \n",
    "\n",
    ">     from answer_metrics import *\n",
    ">Consultez son aide par   \n",
    "\n",
    ">      help(answer_metrics)\n",
    "\n",
    ">Vous pouvez regarder comment elle est faite par \n",
    "\n",
    ">      %load anwer_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> calculer le nombre de lignes total, de lignes de code, de nombre de liens sur la chaîne `tst_texte` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from answer_metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# %load answer_metrics.py\n",
    "import re\n",
    "class answer_metrics():\n",
    "    \"\"\"\n",
    "     Calcul de tout un tas de métriques associées à un texte. \n",
    "     'av_words' : nombre moyen de mots par phrase\n",
    "     'code' :  code contenu dans le texte\n",
    "     'extract_code' : métode pour extraire le code\n",
    "     'html' :  texte initial\n",
    "     'len_html' : longueur du html\n",
    "     'nb_allcaps' : nombre de mots en majuscules\n",
    "     'nb_codelines' : nombre de lignes de code\n",
    "     'nb_imgs' : nombre d'images\n",
    "     'nb_lines' : nombre de lignes\n",
    "     'nb_links' : nombre de liens\n",
    "     'nb_paragraphs' : nombre de paragraphes\n",
    "     'nb_pretty' : nombre de mises en forme gras, italique, souligné,  \n",
    "     'nb_words' : nombre de mots\n",
    "     'strip_code' : méthode pour retirer le code\n",
    "     'striphtml' : méthode pour retirer le texte html\n",
    "     'text' : le texte sans html\n",
    "     =================================================\n",
    "     auteur: jfb mars 2015\n",
    "    \n",
    "    \"\"\"\n",
    "    import re\n",
    "    def __init__(self,text=None):\n",
    "        if text is None: \n",
    "            print(\"You  need to feed the object with a text\")\n",
    "        else:    \n",
    "            self.html=text\n",
    "            self.code=self.extract_code()\n",
    "            self.text=self.striphtml(self.strip_code())\n",
    "        \n",
    "    def __call__(self,text=None):\n",
    "        if text is None: \n",
    "            print(\"You  must feed the object with a text\")\n",
    "        else:    \n",
    "            self.html=text\n",
    "            self.code=self.extract_code()\n",
    "            self.text=self.striphtml(self.strip_code())                    \n",
    "\n",
    "    def striphtml(self,text):\n",
    "        return re.sub('<[^<]+?>', '', text)\n",
    "    \n",
    "    def len_html(self):\n",
    "        return len(self.html) \n",
    "    \n",
    "    def extract_code(self):\n",
    "        out=re.findall('<pre><code>([\\s\\S]*?)</code></pre>', self.html)\n",
    "        return out\n",
    "    \n",
    "    def nb_paragraphs(self):\n",
    "        nb_p=len(re.findall(\"<p>\",self.html))\n",
    "        return nb_p\n",
    "    \n",
    "    def strip_code(self):\n",
    "        out=re.sub('<pre><code>([\\s\\S]*?)</code></pre>', '',self.html)\n",
    "        return out\n",
    "\n",
    "    def nb_codelines(self):\n",
    "        Lcode=0\n",
    "        for code in self.code:\n",
    "            Lcode=Lcode+len(code.split('\\n'))-1 #not perfect. Fails on expression involving \\n\n",
    "        return Lcode\n",
    "\n",
    "    def nb_lines(self):\n",
    "        Llines=len(self.text.split('\\n'))\n",
    "        return Llines\n",
    "\n",
    "    def nb_allcaps(self):\n",
    "        allcaps=re.findall('\\\\b[A-Z]{2,}\\\\b',self.text)\n",
    "        Lallcaps=len(allcaps)\n",
    "        return Lallcaps\n",
    "\n",
    "    def nb_words(self):\n",
    "        Nbwords=len(self.text.split())\n",
    "        return Nbwords\n",
    "\n",
    "    def av_words(self):\n",
    "        sentences=re.split(\"\\\\w[\\.!?]\",self.text)\n",
    "        nb_sentences=len(sentences)\n",
    "        nb_words=0\n",
    "        for sentence in sentences: nb_words+=len(sentence.split())\n",
    "        return nb_words, nb_words/nb_sentences    \n",
    "    \n",
    "    def nb_pretty(self):\n",
    "        return len(re.findall('<strong>',self.html))+\\\n",
    "                len(re.findall('<li>',self.html))+\\\n",
    "                len(re.findall('<em>',self.html))\n",
    "\n",
    "    def nb_links(self):\n",
    "        links=re.findall('<a href=\"http://.*?\".*?>(.*?)</a>',self.html)\n",
    "        nb_links=len(links)\n",
    "        return nb_links\n",
    "\n",
    "    def nb_imgs(self):\n",
    "        imgs=re.findall('<img(.*?)/>',self.html)\n",
    "        nb_imgs=len(imgs)\n",
    "        return nb_imgs    \n",
    "\n",
    "#a=answer_metrics (tst_texte)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Calcul des métriques pour l'ensemble des réponses de la base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Ceci en main, il est possible d'associer un ensemble de métriques à chacune des réponses. Il faut donc relire tous les fichiers csv dans lesquels on a stocké toutes les réponses ; et pour chacune calculer les métriques, noter le score obtenu par la réponse et si elle a été acceptée. Tout ceci sera ensuite conservé dans un grand tableau, pour usage ultérieur. \n",
    "\n",
    "Comme précédemment, on ne va pas vous le faire faire car cela prend pas mal de temps, et ce n'est pas l'objectif principal du cours. \n",
    "\n",
    "Les fonctions suivantes sont donc fournies **à titre de documentation**. Les suggestions d'amélioration sont bienvenues. Vous êtes cependant invités à parcourir le code pour comprendre ce qui se passe. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Les deux routines suivantes font ce travail là. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "```python\n",
    "def zaza(qa_file):\n",
    "    for k in range(nb):\n",
    "        accepted=df2['accepted'][k]\n",
    "        if np.isnan(accepted): accepted=0\n",
    "            \n",
    "        ans_count=np.min([df2['answer_count'][k], 6]) # we just have stocked a maximum of 6 answers\n",
    "        if np.isnan(accepted): accepted=0\n",
    "        all_ans_html=[df2['answer{:d}_body'.format(n)][k] \\\n",
    "                               for n in range(ans_count)]\n",
    "        all_ans_score=[df2['answer{:d}_score'.format(n)][k] \\\n",
    "                               for n in range(ans_count)]\n",
    "        \n",
    "        yield all_ans_html, all_ans_score, accepted\n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Votre serviteur a fait un certain travail : récupéré toutes les questions réponses sur le thème \"python\", entre 2007 et 2015. Il a étiqueté tout cela sous la forme d'un énorme tableau. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "nb_features=10\n",
    "BigA=np.empty((0,nb_features+2))\n",
    "BigNA=np.empty((0,nb_features+2))\n",
    "\n",
    "#Liste de tous les fichiers mensuels de 2008 à 2015\n",
    "qa_list=['qa_python_{0:}-{1:02d}-01.csv'.format(y,m) for y in \n",
    "         [2007,2008,2009,2010, 2011, 2012,2013,2014,2015] for m in range(1,13)]\n",
    "\n",
    "for qa_file in qa_list:\n",
    "    print(\"Processing \",qa_file)\n",
    "    try:\n",
    "        df2=pd.read_csv(qa_file)\n",
    "    except:\n",
    "        print(\"read error\")\n",
    "        continue\n",
    "        \n",
    "    nb=df2.count()['answer_count']\n",
    "    A=np.empty((5*nb,nb_features+2))\n",
    "    NA=np.zeros((5*nb,nb_features+2)); #NA.fill(np.nan)\n",
    "    nn=-1\n",
    "    mm=-1\n",
    "    \n",
    "    for n,tt in enumerate(zaza(qa_file)):\n",
    "        all_ans_html, all_ans_score, accepted = tt\n",
    "        for k,m in enumerate(all_ans_html):\n",
    "            print(\"k\",k,\"accepted\", accepted)\n",
    "            a=answer_metrics (m) \n",
    "            score= all_ans_score[k]\n",
    "            accept=1 if k==accepted else 0\n",
    "            print(\"accept\",accept)\n",
    "            if score>-10003:# was >20  or k==accepted:\n",
    "                mm=mm+1\n",
    "                A[mm,0:nb_features+2]=np.array([a.nb_allcaps(), a.nb_codelines(), a.nb_paragraphs(), \n",
    "                           a.nb_words(), a.nb_lines(), a.nb_links(), a.nb_imgs(), a.av_words()[1],\n",
    "                           a.len_html(), a.nb_pretty(), score, accept])\n",
    "            else:\n",
    "                nn=nn+1\n",
    "                NA[nn,0:nb_features+2]=np.array([a.nb_allcaps(), a.nb_codelines(), a.nb_paragraphs(), \n",
    "                   a.nb_words(), a.nb_lines(), a.nb_links(), a.nb_imgs(), a.av_words()[1],\n",
    "                   a.len_html(), a.nb_pretty(), score, accept])\n",
    "    \n",
    "    if mm!=-1: BigA=np.concatenate((BigA, A[:mm,:]))        \n",
    "    if nn!=-1: BigNA=np.concatenate((BigNA, NA[:nn,:]))  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Sauvegarde du résultat\n",
    "\"\"\"\n",
    "# au format pickle\n",
    "import pickle\n",
    "with open(\"A_all_withaccepted.pkl\", \"wb\") as f: \n",
    "    pickle.dump(BigA,f)\n",
    "# et\n",
    "#au format csv en passant par pandas\n",
    "Acols = ['nb_allcaps', 'nb_codelines', 'nb_paragraphs', 'nb_words', 'nb_lines', \n",
    "         'nb_links', 'nb_imgs', 'av_words', 'len_html', 'nb_pretty', 'score', 'accepted']\n",
    "AA=pd.DataFrame(BigA), columns=Acols)\n",
    "AA.to_csv(\"A_all_withaccepted.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "## Troisième partie: discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "On va maintenant pouvoir effectuer les tâches de discrimination envisagées au début du texte. Pour cela on commence par importer la table et on analyse les principales caractéristiques de la chose. On créera ensuite une colonne cible 'target' selon le problème à traiter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> - Importer la table contenue dans le fichier \"A_all_withaccepted.csv\" par un `pd.read_csv(...)`. \n",
    "Regardez les principales statistiques par un `nom_variable.describe()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feature= [\"allcaps\", \"nb_codelines\", \"nb_paragraphs\", \n",
    "            \"nb_words\", \"nb_lines\", \"nb_links\", \"nb_imgs\", \"av_words\", \"len_html\", \"pretty\"]\n",
    "cols=feature+['score', 'accepted']\n",
    "#évidemment on ne met ni le score ni accepted dans les \"features\" sinon ce serait un peu facile..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "On normalise les features, c'est-à-dire qu'on retire la moyenne et qu'on normalise par l'écart type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "dataset=AA\n",
    "##feature normalization - ça permet de gagner 1/2 %\n",
    "for feat_col in cols[:-2]:\n",
    "    m=dataset[feat_col].mean()\n",
    "    s=dataset[feat_col].std()\n",
    "    print(feat_col,m,s)\n",
    "    #dotaset[feat_col]=dataset[feat_col].apply(lambda x: (x-m)/s)\n",
    "    dataset[feat_col]=(dataset[feat_col]-m)/s\n",
    "    dataset[feat_col+'2']=dataset[feat_col]**2#.apply(lambda x: x**2)\n",
    "dataset.describe()   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Première question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "- <span style=\"color:blue\"> Est-il possible de discriminer les réponses acceptées des non-acceptées ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Travail préparatoire - représentation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Il n'est pas forcément nécessaire d'optimiser sur 400000 échantillons. On peut tirer au hasard un tableau plus petit. Cela se ferait ici de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "AAnew=AA.loc[np.random.choice(np.shape(AA)[0],size=50000, replace=False)]  # pour un dataframe\n",
    "##Anew=A[np.random.choice(np.shape(A)[0],size=20000, replace=False),:] #pour un array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "On ajoute une colonne target qu'on place à 1 si la réponse est acceptée à 0 sinon (identique à accepted et donc un peu superflu...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> Le tableau sur lequel vous allez travailler est `dataset`. Ajouter une colonne 'target' que vous placerez à 1 si la réponse est acceptée à 0 sinon (identique à 'accepted' et donc un peu superflu, mais on aime bien que la cible s'appelle target...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> Vous débuterez par une analyse graphique de la situation. On crée un tableau Pos qui contient les données pour la classe positive et un tableau Neg pour la classe négative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### Indices des réponses acceptées\n",
    "I=AAnew['accepted']==1\n",
    "Pos=AAnew[I]\n",
    "Neg=AAnew[~I]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Voici un exemple de représentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "# Cette bibliothèque permet d'intégrer les images avec un support dynamique javascript \n",
    "#(bibliothèque D3.js) http://fr.wikipedia.org/wiki/D3.js\n",
    "# ce qui permet des zooms dans la version html des pages comme dans le notebook. \n",
    "import mpld3\n",
    "mpld3.enable_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "feature_name='nb_codelines'\n",
    "rmax=100\n",
    "plt.figure()\n",
    "Pos[feature_name].hist(range=[0,rmax],bins=40,alpha=0.5,label=\"positive class\")\n",
    "Neg[feature_name].hist(range=[0,rmax],bins=40,alpha=0.5,label=\"negative class\")\n",
    "plt.title(feature_name)\n",
    "_=plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">A partir de là, tracez l'ensemble des histogrammes pour chacun des features (faites une boucle). Vous pouvez récupérer la liste des features par `feature=list(Pos.columns)`.  Pour chacun des histogrammes, vous pouvez définir une valeur max pour l'abscisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Sympathiquement, on vous donne la liste des rmax pour les abscisses, sous la forme d'un dico\n",
    "feature=list(Pos.columns)\n",
    "feature=feature[1:-3]   #on supprime les trois dernières colonnes \n",
    "                        #qui sont le score, accepted et target\n",
    "r=[15, 40, 40, 500, 80, 10, 10, 40, 800,10,40,2]\n",
    "dico_rmax={ f:r[k] for k,f in enumerate(feature)}\n",
    "print(dico_rmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "> A vous de jouer pour les autres variables explicatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "#### Discrimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Pour utiliser la régression logistique, nous aurons besoin de la classe `LogisticRegression`, d'une méthode découpant en un ensemble d'apprentissage et de test, `train_test_split` et éventuellement des méthodes d'analyse et rapport `confusion_matrix`, `classification_report`. \n",
    "\n",
    "Celles-ci sont importées par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">- Découper le tableau dataset en une table `features` (sélectionner les colonnes intéressantes, à l'aide de la liste des features `feature`), et une table `target` qui extrait la colonne cible. \n",
    "- définir une base de test et une base d'apprentissage en utilisant la méthode `train_test_split(features, target)`\n",
    "- instancier la classe `LogisticRegression`, par exemple sous le nom `cls`, puis apprendre par la méthode `fit` et prédire par `predict`. \n",
    "\n",
    ">Vous pouvez voir quels sont les coefficients obtenus par `cls.intercept_` et `cls.coef_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "On utilisera ensuite les deux routines maison `mycfm` et `classif_eval` pour évaluer les performances. Ces deux routines peuvent être chargées par \n",
    "\n",
    "    from mycfm import *\n",
    "    from classif_eval import *\n",
    "Vous pouvez également, dans le notebook, faire un \n",
    "\n",
    "    %load mycfm.py\n",
    "ce qui vous permet de consulter le source (et donc l'aide).     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "from mycfm import *\n",
    "from classif_eval import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%load mycfm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">Il vous faut donc générer les prédictions des classes. Celles-ci sont directement retournées par `predictions = cls.predict(features_test)`, pour un seuil de 0.5. Vous pouvez également ajuster vous même le seuil en utilisant la méthode `cls.predict_proba(features_test)[:,1]` dont vous comparerez la sortie à une proba seuil. \n",
    "\n",
    "> - générer la matrice de confusion. Examiner les performances obtenues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Donc en gros score de 60%, précision de 60% pour un recall un petit peu < 0.5. Ce n'est pas extraordinaire, mais pas insensé. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">- Qu'obtiendrait-on en classant au hasard ? \n",
    "\n",
    ">En faisant varier le seuil de décision, on va examiner ce que l'on peut obtenir. Pour cela, vous utiliserez la fonction `classif_eval` qui rend les différents taux. \n",
    "\n",
    ">- Vous tracerez alors les courbes précision/recall/spécificité, le score, en fonction du seuil et enfin la courbe ROC. Vous pourrez utiliser ce qu'on a fait en cours [(ici)](http://perso.esiee.fr/~bercherj/IT3007/R%C3%A9gression_logistique.html#Evaluation-des-performances,-matrice-de-confusion) comme modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "### Seconde question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "On s'intéresse maintenant à une question un peu différente. Est-on capable de discriminer les réponses qui ont un peu d'intérêt, de score >0 des réponses pas bonnes (score < 0) ou pas intéressantes (score=0). \n",
    "\n",
    "- <span style=\"color:blue\"> est-il possible de discriminer les réponses correctes (note >0) des autres ?</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "C'est exactement la  même histoire que précédemment, on modifie simplement la cible `target` par le résultat d'un test sur le score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "On le fait pour vous. Vous devez comprendre. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# sous échantillonnage de la grosse table\n",
    "AAnew=AA.loc[np.random.choice(np.shape(AA)[0],size=100000, replace=False)]  # pour un dataframe\n",
    "dataset=AAnew\n",
    "# définition de la cible\n",
    "dataset['target']=dataset['score']>0\n",
    "# mapping car sklearn veut des nombres\n",
    "dataset['target']=dataset['target'].map({True:1, False:0})\n",
    "# on affiche le début de la table pour vérifier\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    ">- implanter une régression logistique pour ce problème,\n",
    "- examinez la matrice de confusion, \n",
    "- tracer les courbes de perf. \n",
    "\n",
    ">Commenter sur les performances. Optimiser les paramètres pour trouver une précision aussi grande que possible, un taux de vrais négatifs > 40%, un recall (taux de vrais positifs) > 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Régression logistique\n",
    "#\n",
    "# A vous de jouer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# calcul et affichage de la matrice de confusion\n",
    "#\n",
    "# A vous de jouer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Courbes de performances\n",
    "#\n",
    "# A vous de jouer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#Sélection d'un seuil et affichage de matrice de confusion correspondant aux perfs pour ce seuil\n",
    "#\n",
    "# A vous de jouer\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0,
   "hotkeys": {
    "equation": "ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "fr",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#39ff00"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "245px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": "4",
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_section_display": "block",
   "toc_threshold": 4,
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
